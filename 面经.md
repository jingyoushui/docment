面经

1.LinkedHashMap底层实现原理

LinkedhashMap是hashMap的子类，保留了插入的顺序，内部维护了一个双重链接列表，在此链表上定义了迭代的顺序。

根据链表中元素的顺序可以分为：按插入顺序的链表，和按访问顺序(调用get方法)的链表。 

默认是按插入顺序排序，如果指定按访问顺序排序，那么调用get方法后，会将这次访问的元素移至链表尾部，不断访问可以形成按访问顺序排序的链表。  可以重写removeEldestEntry方法返回true值指定插入元素时移除最老的元素。

![Image(image/%E9%9D%A2%E7%BB%8F/799093-20170923233422087-1290290404.png)](https://images2017.cnblogs.com/blog/799093/201709/799093-20170923233422087-1290290404.png)

![Image(image/%E9%9D%A2%E7%BB%8F/799093-20170923233424384-870091534.png)](https://images2017.cnblogs.com/blog/799093/201709/799093-20170923233424384-870091534.png)



因为LinkedHashMap由于其本身维护了插入的先后顺序，因此LinkedHashMap可以用来做缓存，

**1.从table的角度看，新的entry需要插入到对应的bucket里，当有哈希冲突时，采用头插法将新的entry插入到冲突链表的头部。**
**2.从header的角度看，新的entry需要插入到双向链表的尾部。**

LinkedHashMap的实现就是HashMap+LinkedList的实现方式，以HashMap维护数据结构，以LinkList的方式维护数据插入顺序。



**LinkedHashMap读取元素**

LinkedHashMap重写了父类HashMap的get方法，实际在调用父类getEntry()方法取得查找的元素后，再判断当排序模式accessOrder为true时（即按访问顺序排序），先将当前节点从链表中移除，然后再将当前节点插入到链表尾部。由于的链表的增加、删除操作是常量级的，故并不会带来性能的损失。

**利用LinkedHashMap实现LRU算法缓存**

```java
public class LRUCache extends LinkedHashMap
{
    public LRUCache(int maxSize)
    {
        super(maxSize, 0.75F, true);//accessOrder为true时（即按访问顺序排序
        maxElements = maxSize;
    }

    protected boolean removeEldestEntry(java.util.Map.Entry eldest)
    {
        return size() > maxElements;
    }

    private static final long serialVersionUID = 1L;
    protected int maxElements;
}

public LinkedHashMap(int initialCapacity,
         float loadFactor,
                     boolean accessOrder) {
    super(initialCapacity, loadFactor);
    this.accessOrder = accessOrder;
}
```

LinkedHashMap可以实现LRU算法的缓存基于两点：

1、LinkedList首先它是一个Map，Map是基于K-V的，和缓存一致

2、LinkedList提供了一个boolean值可以让用户指定是否实现LRU

**LRU即Least Recently Used，最近最少使用，也就是说，当缓存满了，会优先淘汰那些最近最不常访问的数据**

第二点的意思就是，如果有1 2 3这3个Entry，那么访问了1，就把1移到尾部去，即2 3 1。每次访问都把访问的那个数据移到双向队列的尾部去，那么每次要淘汰数据的时候，双向队列最头的那个数据不就是最不常访问的那个数据了吗？换句话说，双向链表最头的那个数据就是要淘汰的数据。

看到每次recordAccess的时候做了两件事情：

1、把待移动的Entry的前后Entry相连

2、把待移动的Entry移动到尾部





## Zset底层数据结构

zset底层的存储结构包括ziplist或skiplist，在同时满足以下两个条件的时候使用ziplist，其他时候使用skiplist，两个条件如下：

- 有序集合保存的元素数量小于128个
- 有序集合保存的所有元素的长度小于64字节

 当ziplist作为zset的底层存储结构时候，每个集合元素使用两个紧挨在一起的压缩列表节点来保存，第一个节点保存元素的成员，第二个元素保存元素的分值。

 当skiplist作为zset的底层存储结构的时候，使用skiplist按序保存元素及分值，使用dict来保存元素和分值的映射关系。

![img](image/%E9%9D%A2%E7%BB%8F/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy82OTkwMDM1LTRkODU5YzI1ZGY3NjM5M2UucG5nP2ltYWdlTW9ncjIvYXV0by1vcmllbnQvc3RyaXB8aW1hZ2VWaWV3Mi8yL3cvNzcxL2Zvcm1hdC93ZWJw.jpeg)

 skiplist作为zset的存储结构，整体存储结构如下图，核心点主要是包括一个dict对象和一个skiplist对象。dict保存key/value，key为元素，value为分值；skiplist保存的有序的元素列表，每个元素包括元素和分值。两种数据结构下的元素指向相同的位置。

一个 zset 结构同时包含一个字典和一个跳跃表：

```java
typedef struct zset{
     //跳跃表
     zskiplist *zsl;
     //字典
     dict *dice;
} zset;
```

字典的键保存元素的值，字典的值则保存元素的分值；跳跃表节点的 object 属性保存元素的成员，跳跃表节点的 score 属性保存元素的分值。

这两种数据结构会通过指针来共享相同元素的成员和分值，所以不会产生重复成员和分值，造成内存的浪费。

说明：其实有序集合单独使用字典或跳跃表其中一种数据结构都可以实现，但是这里使用两种数据结构组合起来，原因是假如我们单独使用 字典，虽然能以 O(1) 的时间复杂度查找成员的分值，但是因为字典是以无序的方式来保存集合元素，所以每次进行范围操作的时候都要进行排序；假如我们单独使用跳跃表来实现，虽然能执行范围操作，但是查找操作有 O(1)的复杂度变为了O(logN)。因此Redis使用了两种数据结构来共同实现有序集合。


## HashMap扩容原理

扩容的时机：当元素个数达到扩容因子×数组桶的长度时候，**当`map`中包含的`Entry`的数量大于等于`threshold = loadFactor \* capacity`的时候，且新建的`Entry`刚好落在一个非空的桶上，此刻触发扩容机制，将其容量扩大为2倍。**当`size`大于等于`threshold`的时候，并不一定会触发扩容机制，但是会很可能就触发扩容机制，只要有一个新建的`Entry`出现哈希冲突，则立刻`resize`。*因为size 已经大于等于阈值了，说明Entry数量较多，哈希冲突严重，那么若该Entry对应的桶不是一个空桶，这个Entry的加入必然会把原来的链表拉得更长，因此需要扩容；若对应的桶是一个空桶，那么此时没有必要扩容*.

扩容过程：首先将容量扩大到两倍，是新建了一个数组，然后判断元素是否需要再hash,计算元素的hash值，将元素复制到新的数组中,*完成旧表到新表的转移*。

*数据转移：顺序遍历某个桶的外挂链表*,*找到新表的桶位置;原桶数组中的某个桶上的同一链表中的Entry此刻可能被分散到不同的桶中去了，有效的缓解了哈希冲突。*在指定的桶位置上，创建一个新的Entry。在1.7中采用的是头插法，1.8采用的是尾插法。头插法会导致扩容的时候链表的顺序颠倒，如果多个线程同时进行扩容操作，将会出现环。



问题：隐约感觉这个transfer方法在多线程环境下会乱套。事实上也是这样的，由于缺乏同步机制，当多个线程同时resize的时候，某个线程t所持有的引用next（参考上面代码next指向原桶数组中某个桶外挂单链表的下一个需要转移的Entry），可能已经被转移到了新桶数组中，那么最后该线程t实际上在对新的桶数组进行transfer操作。

如果有更多的线程出现这种情况，那很可能出现大量线程都在对新桶数组进行transfer，那么就会出现多个线程对同一链表无限进行链表反转的操作，极易造成死循环，数据丢失等等，因此HashMap不是线程安全的

为什么扩容的是原来的两倍：为了性能。在`HashMap`通过键的哈希值进行定位桶位置的时候，调用了一个`indexFor(hash, table.length);`方法。可以看到这里是将哈希值h与桶数组的length-1（实际上也是map的容量-1）进行了一个与操作得出了对应的桶的位置，h & (length-1)。

但是为什么不采用h % length这种计算方式呢？Java的%、/操作比&慢10倍左右，因此采用&运算会提高性能。通过限制length是一个2的幂数，h & (length-1)和h % length结果是一致的。


如果`length`是一个2的幂的数，那么`length-1`就会变成一个`mask`, 它会将`hashcode`低位取出来，`hashcode`的**低位实际就是余数**，和取余操作相比，与操作会将性能提升很多。不同的键的的`hashcode`仅仅只能通过**低位**来区分。**高位的信息**没有被充分利用。**极端情况就是：所有的`hashCode`低位全相等，而高位不相等**，这大大加大了哈希冲突，降低了`HashMap`的性能。



链表头插法的会颠倒原来一个散列桶里面链表的顺序。在并发的时候原来的顺序被另外一个线程a颠倒了，而被挂起线程b恢复后拿扩容前的节点和顺序继续完成第一次循环后，又遵循a线程扩容后的链表顺序重新排列链表中的顺序，最终形成了环。

当两个线程分别对hashmap进行插入操作 并且引发了扩展的操作，在这种情况下采用头插法的话 重新扩展后的定位又是在同一个桶的话 那很容易导致成环 死循环出不来 最主要原因是null值的丢失。

如果采用尾插法的话null值不会丢失 就算是多线程导致扩展也不会导致死循环的情况出现 最多只是丢失一部分数据（如果是直接在尾部插入）

1.7：https://cloud.tencent.com/developer/article/1554536

- 头插法，扩容的时候会倒序，多个线程同时扩容会出现环状结构。至于为什么会采用头插法，据说是考虑到热点数据的原因，即最近插入的元素也很可能最近会被使用到。所以为了缩短链表查找元素的时间，所以每次都会将新插入的元素放到表头。

- 扩容造成数据丢失，并形成环状死循环

- put过程：当我们想一个HashMap中添加一对key-value时，系统首先会计算key的hash值，然后根据hash值确认在table中存储的位置。

  若该位置没有元素，则直接插入。否则迭代该处元素链表并依此比较其key的hash值。

  如果两个hash值相等且key值相等(e.hash == hash && ((k = e.key) == key || key.equals(k))),则用新的Entry的value覆盖原来节点的value。

  如果两个hash值相等但key值不等 ，则将该节点插入该链表的**链头**（最先保存的元素放在链尾）,新元素设置为Entry[0]，其next指针指向原有对象，即原有对象为Entry[1]

1.8：

- 尾插法，不会出现环状链表

- 在多线程的环境下，会出现数据覆盖的情况。在put的时候，如果没有hash碰撞则直接插入元素，如果线程A和线程B同时进行put操作，刚好这两条不同的数据hash值一样，并且该位置数据为null，所以这线程A、B都会进入第6行代码中。假设一种情况，线程A进入后还未进行数据插入时挂起，而线程B正常执行，从而正常插入数据，然后线程A获取CPU时间片，此时线程A不用再进行hash判断了，问题出现：线程A会把线程B插入的数据给覆盖，发生线程不安全。

- put:

  ![img](https://images2015.cnblogs.com/blog/443934/201611/443934-20161101205342330-1626693584.png)

## ConcurrentHashMap 1.7

ConcurrentHashMap1.7是使用了分段锁的思想，Segment数组结构和HashEntry数组结构组成。HashEntry是一个数组，每个位置是一个链表。ConcurrentHashMap不会增加Segment的数量，而只会增加Segment中链表数组的容量大小，这样的好处是扩容过程不需要对整个ConcurrentHashMap做rehash，而只需要对Segment里面的元素做一次rehash就可以了。



首先确定放入到哪个Segment：这个函数用了位操作来确定Segment，根据传入的hash值向右无符号右移segmentShift位，然后和segmentMask进行与操作，结合我们之前说的segmentShift和segmentMask的值，就可以得出以下结论：假设Segment的数量是2的n次方，根据元素的hash值的高n位就可以确定元素到底在哪一个Segment中。以再散列值的高位进行取模得到当前元素在哪个segment上。

确定在table中的哪个位置：同样，这里也是用位操作来确定链表的头部，hash值和HashTable的长度减一做与操作，最后的结果就是hash值的低n位，其中n是HashTable的长度以2为底的结果。*同样是取得key的再散列值以后，用再散列值的全部和table的长度进行取模，得到当前元素在table的哪个元素上。

取值：在确定了链表的头部以后，就可以对整个链表进行遍历，看第4行，取出key对应的value的值，如果拿出的value的值是null，则可能这个key，value对正在put的过程中，如果出现这种情况，那么就加锁来保证取出的value是完整的，如果不是null，则直接返回value。get方法没有使用锁来同步，只是判断获取的entry的value是否为null，为null时才使用加锁的方式再次去获取。

首先对Segment的put操作是加锁完成的，然后在第五行，如果Segment中元素的数量超过了阈值（由构造函数中的loadFactor算出）这需要进行对Segment扩容，并且要进行rehash。

ConcurrentHashMap采用的是锁分段技术，内部为Segment数组来进行细分，而每个Segment又通过HashEntry数组来进行组装，当进行写操作的时候，只需要对这个key对应的Segment进行加锁操作，加锁同时不会对其他的Segment造成影响。总的Map包含了16个Segment（默认数量），每个Segment内部包含16个HashEntry（默认数量），这样对于这个key所在的Segment加锁的同时，其他15个Segmeng还能正常使用，在性能上有了大大的提升。

同时ConcurrentHashMap只是针对put方法进行了加锁，而对于get方法并没有采用加锁的操作，因为具体的值，在Segment的HashEntry里面是volatile的，基于happens-before（先行发生）原则，对数据的写先行发生于对数据的读，所以再读取的时候获取到的必然是最新的结果。



1.HashEntry里面的value值是被volatile修饰的，然后HashEntry里面除了value值不是final修饰的，其他都被final修饰了，所以在HashEntry链表里面添加HashEntry的时候，只能添加到头节点，不能添加到尾节点，因为HashEntry里面的next值是被final修饰的，不能修改

2.ConcurrentHashMap的`get`操作时候，如果发现新增，修改，删除都是怎么考虑并发问题的
2.1新增的时候，如果刚好new一个对象，然后此时对象是一个没有完全构造好的对象引用。没有锁同步的话，此时get的时候就有可能获取到一个null的对象，解决办法就是如果HashEntry里面的value是null的时候，在加锁去获取一次
2.2在修改的时候，因为HashEntry里面的value值是被volatile修饰的，所以修改对获取数据没有影响



3.2ConcurrentHashMap定位一个元素的过程需要进行两次Hash操作，第一次Hash定位到Segment，第二次Hash定位到元素所在的链表的头部，这样好处是多个线程可以同时读写操作多个segment，这样就降低了锁的颗粒度

4.ConcurrentHashMap只是针对put方法进行了加锁，而对于get方法并没有采用加锁的操作，因为具体的值，在Segment的HashEntry里面是volatile的

5.Segment数组的count用volatile修饰主要是保证在数组扩容的时候保证可见性。而且是使用了分段CAS来累加计算



用于定位元素所在segment。segmentShift表示偏移位数，通过前面的int类型的位的描述我们可以得知，int类型的数字在变大的过程中，低位总是比高位先填满的，为保证元素在segment级别分布的尽量均匀，计算元素所在segment时，总是取hash值的高位进行计算。segmentMask作用就是为了利用位运算中取模的操作：
a % (Math.pow(2,n)) 等价于 a&( Math.pow(2,n)-1)



put:    segment继承了ReentrantLock

![](image/%E9%9D%A2%E7%BB%8F/jdk7%20concurrenthashmap%20put%E6%B5%81%E7%A8%8B.png)

## ConcurrentHashMap 1.8

让每一个CPU去执行一段数据的扩容，每一个CPU可以处理16个长度的数组。这样就可以并发扩容了。例如每个线程对数组中的长度16的数组进行扩容，扩容到32个，就是新建了一个长度为32的数组，然后将原来在长度为16的数组上的数据转移到新数组上。采用的是CAS+synchronized来保证在多线程的安全。

Node 和HashMap中的Node结构大致类似，只不过value和next通过volatile进行了修饰，保证了内存的可见性。还增加了一个find方法，通过这个node遍历之后的全部node找对对应key的节点。

put过程：

- value不能为空>获取hash值>第一次put的时候对table进行初始化>如果table对应的index上为空，则初始化Node为空>如果当前map在扩容，则先协助扩容>如果hash冲突>遍历查找>如果key相同，直接替换，如果没有相同的key，则添加在链表尾部

#### 转移数据

如何转移数据呢，每个线程进来，首先要确定当前线程对应的table数组的边界，数据迁移的过程是从后往前，判断每一个位置是否有元素，没有元素就向前移动，如果不为空，就会转移数据

假设当前table数组如下所示，两个线程执行了扩容，创建了一个长度为64的数组，

![在这里插入图片描述](image/%E9%9D%A2%E7%BB%8F/20200218175107156.png)

转移过程：

如果某个数组位置上连接的节点有了八个，就需要扩容来解决装不下的问题，首先需要对单向链表上的八个节点进行分类，根据某种分类算法把他们分成两类，一个高位链，一个低位链。

低位链保持不动* ，高位链需要增加n长度的位置*



![在这里插入图片描述](image/%E9%9D%A2%E7%BB%8F/20200218175124966.png)

为什么这么做呢，为的是在新的数组中通过原来的计算还能够取到原来的值，不会使值丢失。扩容以后，同样的操作拿到的值是不会变的。原因如下：

取值的语句如下：

f = tabAt(tab, i = (n - 1) & hash))    
1
下标位为(n - 1) & hash，假设原来的hash值为9，那么它在16位长度的table中表示如下：

0000 1111 & 0000 1001 = 9

在32位的table中呢

0001 1111 & 0000 1001 = 9，说明低位的是不会变的。

对于高位，假设hash为20

16位：0000 1111 & 0001 0100 = 0000 0100 = 4，原本会取4这个位置的元素值

32位：0001 1111 & 0001 0100 = 0001 0100 = 20，同样的计算方式，现在要去取20位置的值

所以对于高位的元素需要加上n，这里n=16。这样高低链来解决多次hash计算的问题，提升了效率。



HashSet内部基于HashMap来实现的，底层采用HashMap来保存元素。Set集合无序并不可重复。HashSet中的元素都存放在HashMap的key上面，而value中的值都是统一的一个private static final Object PRESENT = new Object();。HashSet跟HashMap一样，都是一个存放链表的数组。




## redis中都有哪些数据结构

1.字符串String,底层实现是字符数组

字符串(String)
       与其它编程语言或其它键值存储提供的字符串非常相似，键(key)------值(value) (字符串格式),字符串拥有一些操作命令，如：get set del 还有一些比如自增或自减操作等等。redis是使用C语言开发，但C中并没有字符串类型，只能使用指针或符数组的形式表示一个字符串，所以redis设计了一种简单动态字符串(SDS[Simple Dynamic String])作为底实现：

定义SDS对象，此对象中包含三个属性：

len buf中已经占有的长度(表示此字符串的实际长度)
free buf中未使用的缓冲区长度
buf[] 实际保存字符串数据的地方
所以取字符串的长度的时间复杂度为O(1)，另，buf[]中依然采用了C语言的以\0结尾可以直接使用C语言的部分标准C字符串库函数。

空间分配原则：当len小于IMB（1024*1024）时增加字符串分配空间大小为原来的2倍，当len大于等于1M时每次分配 额外多分配1M的空间。

由此可以得出以下特性：

redis为字符分配空间的次数是小于等于字符串的长度N，而原C语言中的分配原则必为N。降低了分配次数提高了追加速度，代价就是多占用一些内存空间，且这些空间不会自动释放。
二进制安全的
高效的计算字符串长度(时间复杂度为O(1))
高效的追加字符串操作。



2list,底层使用双向链表



列表(List)
         redis对键表的结构支持使得它在键值存储的世界中独树一帜，一个列表结构可以有序地存储多个字符串，拥有例如：lpush lpop rpush rpop等等操作命令。在3.2版本之前，列表是使用ziplist和linkedlist实现的，在这些老版本中，当列表对象同时满足以下两个条件时，列表对象使用ziplist编码：

列表对象保存的所有字符串元素的长度都小于64字节
列表对象保存的元素数量小于512个
当有任一条件 不满足时将会进行一次转码，使用linkedlist。

而在3.2版本之后，重新引入了一个quicklist的数据结构，列表的底层都是由quicklist实现的，它结合了ziplist和linkedlist的优点。按照原文的解释这种数据结构是【A doubly linked list of ziplists】意思就是一个由ziplist组成的双向链表。那么这两种数据结构怎么样结合的呢？

ziplist的结构

         由表头和N个entry节点和压缩列表尾部标识符zlend组成的一个连续的内存块。然后通过一系列的编码规则，提高内存的利用率，主要用于存储整数和比较短的字符串。可以看出在插入和删除元素的时候，都需要对内存进行一次扩展或缩减，还要进行部分数据的移动操作，这样会造成更新效率低下的情况。

这篇文章对ziplist的结构讲的还是比较详细的：

https://blog.csdn.net/yellowriver007/article/details/79021049

linkedlist的结构

        意思为一个双向链表，和普通的链表定义相同，每个entry包含向前向后的指针，当插入或删除元素的时候，只需要对此元素前后指针操作即可。所以插入和删除效率很高。但查询的效率却是O(n)[n为元素的个数]。

了解了上面的这两种数据结构，我们再来看看上面说的“ziplist组成的双向链表”是什么意思？实际上，它整体宏观上就是一个链表结构，只不过每个节点都是以压缩列表ziplist的结构保存着数据，而每个ziplist又可以包含多个entry。也可以说一个quicklist节点保存的是一片数据，而不是一个数据。总结：

整体上quicklist就是一个双向链表结构，和普通的链表操作一样，插入删除效率很高，但查询的效率却是O(n)。不过，这样的链表访问两端的元素的时间复杂度却是O(1)。所以，对list的操作多数都是poll和push。
每个quicklist节点就是一个ziplist，具备压缩列表的特性。
在redis.conf配置文件中，有两个参数可以优化列表：

list-max-ziplist-size 表示每个quicklistNode的字节大小。默认为-2 表示8KB
list-compress-depth 表示quicklistNode节点是否要压缩。默认是0 表示不压缩



hash



哈希(hash)
        redis的散列可以存储多个键 值 对之间的映射，散列存储的值既可以是字符串又可以是数字值，并且用户同样可以对散列存储的数字值执行自增操作或者自减操作。散列可以看作是一个文档或关系数据库里的一行。hash底层的数据结构实现有两种：

一种是ziplist，上面已经提到过。当存储的数据超过配置的阀值时就是转用hashtable的结构。这种转换比较消耗性能，所以应该尽量避免这种转换操作。同时满足以下两个条件时才会使用这种结构：
当键的个数小于hash-max-ziplist-entries（默认512）
当所有值都小于hash-max-ziplist-value（默认64）
另一种就是hashtable。这种结构的时间复杂度为O(1)，但是会消耗比较多的内存空间。



3.set  底层使用的是hashtable.



集合(Set)
         redis的集合和列表都可以存储多个字符串，它们之间的不同在于，列表可以存储多个相同的字符串，而集合则通过使用散列表（hashtable）来保证自已存储的每个字符串都是各不相同的(这些散列表只有键，但没有与键相关联的值)，redis中的集合是无序的。还可能存在另一种集合，那就是intset，它是用于存储整数的有序集合，里面存放同一类型的整数。共有三种整数：int16_t、int32_t、int64_t。查找的时间复杂度为O(logN)，但是插入的时候，有可能会涉及到升级（比如：原来是int16_t的集合，当插入int32_t的整数的时候就会为每个元素升级为int32_t）这时候会对内存重新分配，所以此时的时间复杂度就是O(N)级别的了。注意：intset只支持升级不支持降级操作。

intset在redis.conf中也有一个配置参数set-max-intset-entries默认值为512。表示如果entry的个数小于此值，则可以编码成REDIS_ENCODING_INTSET类型存储，节约内存。否则采用dict的形式存储。




4. zset

   有序集合(zset)
           有序集合和散列一样，都用于存储键值对：有序集合的键被称为成员（member),每个成员都是各不相同的。有序集合的值则被称为分值（score），分值必须为浮点数。有序集合是redis里面唯一一个既可以根据成员访问元素(这一点和散列一样),又可以根据分值以及分值的排列顺序访问元素的结构。它的存储方式也有两种：

   是ziplist结构。
             与上面的hash中的ziplist类似，member和score顺序存放并按score的顺序排列

   另一种是skiplist与dict的结合。
            skiplist是一种跳跃表结构，用于有序集合中快速查找，大多数情况下它的效率与平衡树差不多，但比平衡树实现简单。redis的作者对普通的跳跃表进行了修改，包括添加span\tail\backward指针、score的值可重复这些设计，从而实现排序功能和反向遍历的功能。

   一般跳跃表的实现，主要包含以下几个部分：

   表头（head）：指向头节点
   表尾（tail）：指向尾节点
   节点（node）：实际保存的元素节点，每个节点可以有多层，层数是在创建此节点的时候随机生成的一个数值，而且每一层都是一个指向后面某个节点的指针。
   层（level）：目前表内节点的最大层数
   长度（length）：节点的数量。
   跳跃表的遍历总是从高层开始，然后随着元素值范围的缩小，慢慢降低到低层。





## 自我介绍

面试官您好，我是李行行，今年24岁，应聘的岗位是后端开发，

我的本科是计算机专业，就读于上海电力大学，本科期间主要学习了数据结构，计算机网络，操作系统还有java，然后我自学过两年python。本科期间做过一次校创，主要开发的是学校的一个失物招领的网站，分为web端和微信小程序，主要是我一个人开发的。然后参加过一次创青春的比赛，做的是爬虫和数据可视化的网站，主要是分析各大网站上的租房信息的。爬虫框架是Scrapy.



然后我目前是在南京大学软件学院就读，从去年九月份就跟着项目组在天津市法院做研发。我在这边参与的法院的项目是一个法院里的测评系统，就是每个月会有职工的评选。使用的就是springboot+vue的前后分离开发的。提供了7种常用的测评表，也支持自定义表单，就和腾讯文档的表单类似。数据库使用了mysql和mongodb，因为有一些非关系的数据需要保存到Mongodb中，比如创建表单的json数据。使用了redis做缓存。



另外我负责的一个项目是一个外包项目，工业上的画图网站，主要是绘制布线图的。使用的是springboot+angular前后端分离。同样的使用到了mysql,mongodb,redis,还有docker,nginx。这个项目目前还不是分布式的，因为上线初期并发量不会太大。





## MongoDB如何保证事务

MongoDB在更新单个文档时，会对该文档加锁，

如果当`age`修改成功，而`score`没有修改成功时，MongoDB会自动回滚，因此我们可以说针对单个文档，MongoDB是支持事务，保证ACID的

所有的锁都是平等的，它们是排在一个队列里，符合FIFO原则。但是，MongoDB做了优化，即当一个锁被采用时，所有与它兼容的锁（即上表为yes的锁）都会被采纳，从而可以并发操作。

简单来说，只要不是同一个Document，读写操作是可以并发的；如果是同一个Document，读可以并发，但写不可以。

任何一步失败都有相应的应对措施来保证事务或者执行完毕或者回滚。当然所有的实现都需要应用程序自己实现，更何况如果涉及多个应用并发的情况时，会更加复杂，如何保证多个事务不互相影响，又会进一步增加复杂度，这也就是为什么如果需要此类跨文档事务支持的时候推荐使用关系数据库。

当我们说外部一致性时，是针对分布式系统所说的CAP理论中的一致性，简单来说就是如何使得多台机器的副本保持一致，实际上Mongodb只能做到最终一致性，当我们说内部一致性时，是针对ACID中的一致性，这里主要针对如何避免脏读，当Mongodb无法在大多数节点成功的更新操作时，会导致回滚操作，这时如果Reader已经读取了更改后的数据，就会产生脏读现象。而避免脏读，可以通过设置Read Concern和Write Concer来实现，

解决方案一：字段同步。一个常见的办法是利用文档的性质：不需要很多行、很多关系，你可以将所有的东西嵌入到一个大文档中，Denormalization将带你回归事务。

解决方案二：作业队列。

现在A和B决定成为朋友：你需要把B添加到A的列表，也需要把A添加到B的列表。如果两者没有同时发生也没有关系（只要没有引发困扰）。针对这种情况和大多数事务问题的解决方案是使用作业队列，作业队列也存储在MongoDB。一个作业文档就像这样：

```
{ _id: jobId, ts: timeStamp, state: "TODO", type: "ADD_FRIEND", details: { users: [ userA, userB ]} }
```

方案三：两阶段提交

方案四： Log Reconciliation

很多财务系统常用的解决方案是 log reconciliation。这种解决方案将事务写作简单的日志，这避免了复杂性和潜在的故障。执行事务时，一个典型的财务系统会给事务写一个条目，会给与事务有关的账户写一个“账户变化”条目。这个方法需要进一步的写保证，“作业队列”解决方案可以实现写保证，事务中所有的作业在所有账户更改写入前都会保持不变。

方案五：版本控制

有时变得很复杂，以至于不能再JSON中表示，这些变更可能涉及很多有着复杂关系的文件（如树结构）。如果仅是部分变化（如破坏树）将会很混乱，这种情况下我们需要隔离。获取隔离性的一种方式是插入有着高版本号的新文档，取代对现有文档的更新。可以通过同日志和解同样的技术很容易、很安全的获得新版本号。通常{ itemId: 1, version: 1}上有一个独特的索引。



## python中的线程和进程

1.线程：无法实现跑多核cpu，在IO密集型的情况下(如读写文件，可以通过多线程增加其工作效率)，建议使用多线程；多线程相比单线程性能有提升，因为遇到IO阻塞会自动释放GIL锁，实现无缝连接执行下一个线程

 

解释：

因为Python的线程虽然是真正的线程，但解释器执行代码时，有一个GIL锁：

Global Interpreter Lock，任何Python线程执行前，必须先获得GIL锁，然后，每执行100条字节码，解释器就自动释放GIL锁，让别的线程有机会执行。

这个GIL全局锁实际上把所有线程的执行代码都给上了锁，所以，多线程在Python中只能交替执行，即使100个线程跑在100核CPU上，也只能用到1个核。

GIL是Python解释器设计的历史遗留问题，通常我们用的解释器是官方实现的CPython，要真正利用多核，除非重写一个不带GIL的解释器。

所以，在Python中，可以使用多线程，但不要指望能有效利用多核。如果一定要通过多线程利用多核，那只能通过C扩展来实现，不过这样就失去了Python简单易用的特点。

 


在多线程环境下，每个线程都有自己的数据。一个线程使用自己的局部变量比使用全局变量好，因为局部变量只有线程自己能看见，不会影响其他线程，但每个函数都使用局部变量会造成很多的困扰，因此解决方案有两种


线程数据共享书问题解决方案：加锁或者创建threading对象

创建全局ThreadLocal对象:该对象可以在共享同一全部变量时实现任意读写而互不干扰，因此免去了锁管理烦恼，ThreadLocal方法内部做了处理

local_school = threading.local()

 

2.进程：可以实现跑多核cpu，如果是CPU密集型的任务，建议使用多进程

解释：多个Python进程有各自独立的GIL锁，互不影响。一个进程有一个 GIL 

 

3.协程
单线程的异步编程模型称为协程，有了协程的支持，就可以基于事件驱动编写高效的多任务程序。
https://www.cnblogs.com/Presley-lpc/p/10054633.html

分布式进程使用参考：(可以试试一台服务器使用队列存，另一台服务器取，在不同服务器上完成操作)
https://www.liaoxuefeng.com/wiki/1016959663602400/1017631559645600





Java中Vector和ArrayList的区别](https://www.cnblogs.com/wanlipeng/archive/2010/10/21/1857791.html)

   首先看这两类都实现List接口，而List接口一共有三个实现类，分别是ArrayList、Vector和LinkedList。List用于存放多个元素，能够维护元素的次序，并且允许元素的重复。3个具体实现类的相关区别如下：

1. ArrayList是最常用的List实现类，内部是通过数组实现的，它允许对元素进行快速随机访问。数组的缺点是每个元素之间不能有间隔，当数组大小不满足时需要增加存储能力，就要讲已经有数组的数据复制到新的存储空间中。当从ArrayList的中间位置插入或者删除元素时，需要对数组进行复制、移动、代价比较高。因此，它适合随机查找和遍历，不适合插入和删除。
2. Vector与ArrayList一样，也是通过数组实现的，不同的是它支持线程的同步，即某一时刻只有一个线程能够写Vector，避免多线程同时写而引起的不一致性，但实现同步需要很高的花费，因此，访问它比访问ArrayList慢。
3. LinkedList是用链表结构存储数据的，很适合数据的动态插入和删除，随机访问和遍历速度比较慢。另外，他还提供了List接口中没有定义的方法，专门用于操作表头和表尾元素，可以当作堆栈、队列和双向队列使用。

关于ArrayList和Vector区别如下：

1. ArrayList在内存不够时默认是扩展50% + 1个，Vector是默认扩展1倍。
2. Vector提供indexOf(obj, start)接口，ArrayList没有。
3. Vector属于线程安全级别的，但是大多数情况下不使用Vector，因为线程安全需要更大的系统开销。







## Innodb四大特性

1.插入缓冲：只对非聚集索引的插入和更新有效，对于每一次的插入不是写入索引页中，而是先判断插入的非聚集索引页是否在缓冲池中，如果在则直接插入，如果不在，则先放入到insert buffer中，再按照一定的频率进行合并操作，再写回disk。这样能够将多个插入合并到一个操作中，目的是减少随机IO带来的性能损耗。

2.二次写：二次写缓存是位于系统表空间的存储区域，当操作系统或者数据库进程在数据页写磁盘的过程中崩溃，innodb可以在二次写缓存中找到数据备份。

3.自适应哈希索引

innodb存储引擎会监控对表上二级索引的查找，如果发现某二级索引被频繁访问，二级索引成为热点数据，建立hash索引可以提升速度。

经常访问的二级索引数据会自动被生成到hash索引里面去(最近连续被访问三次的数据)，自适应哈希索引通过缓冲池的B+树构造而来，因此建立的速度很快。
哈希（hash）是一种非常快的等值查找方法，在一般情况下这种查找的时间复杂度为O(1),即一般仅需要一次查找就能定位数据。而B+树的查找次数，取决于B+树的高度，在生产环境中，B+树的高度一般3-4层，故需要3-4次的查询。

4.预读：线性预读和随机预读

线性预读：当访问到extent的最后一个page的时候，Innodb会决定是否将下一个extent放入到buffer pool中。

随机预读：随机预读方式则是表示当同一个extent中的一些page在buffer pool中发现时，Innodb会将该extent中的剩余page一并读到buffer pool中



## 单点登录的问题

后端随机生成token，将token作为key，用户信息作为value存到redis中。

在每个子项目中都添加一个配置类或者直接设置cookie的路径，如果有域名还可以设置域名的限制，比如 set.xxx.com 与 get.xxx.com 这种情况与我们就需要设置cookie的域名为 xxx.com，以确保无法在哪个项目下都能够获取 xxx.com 这个域名下的cookie值。这样就确保能够正常获得共享的session值了。

```java
@Bean
public static DefaultCookieSerializer defaultCookieSerializer() {
    DefaultCookieSerializer serializer = new DefaultCookieSerializer();
    serializer.setCookiePath("/");
    //serializer.setDomainName("xxx.com"); //如果使用域名访问，建议对这一句进行设置    
    return serializer;
}
```

session中保存用户的什么信息？用户名，用户ID

过期时间？更新？

登出的时候将redis中的key删除

拦截器implements HandlerInterceptor，

```java
/**
     * 预处理回调方法,实现处理器的预处理，第三个参数object为响应的处理器,自定义Controller,返回值为true表示继续流程（如调用下一个拦截器或处理器）或者接着执行
     * postHandle()和afterCompletion()；false表示流程中断，不会继续调用其他的拦截器或处理器，中断执行。
     *
     * 主要流程:
     * 1.从 http 请求头中取出 token
     
     * 2.判断是否映射到方法,没有映射到方法直接返回true放行
     
             if (!(object instanceof HandlerMethod)) {
                    return true;
                }
                
     * 3.检查是否有passtoken注释，有则跳过认证
 		    method.isAnnotationPresent(PassToken.class)
     
     * 4.检查有没有需要用户登录的注解，有则需要取出并验证
             if (method.isAnnotationPresent(UserLoginToken.class)) 
             如果token为Null,则重新登录，前端跳转到登录页面
             有token，则获取到token去redis中查找，查找之后更新过期时间
     
     * 5.认证通过则可以访问，并向浏览器中种cookie，不通过会报相关错误信息
     *
     * */
拦截器去获取token并验证token
 public boolean preHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object object)
```

```java
 * @Description:跳过验证的注解
 */
@Target({ElementType.METHOD, ElementType.TYPE})
@Retention(RetentionPolicy.RUNTIME)
public @interface PassToken {
    boolean required() default true;
}


 * @Description:需要验证的注解
 */
@Target({ElementType.METHOD, ElementType.TYPE})
@Retention(RetentionPolicy.RUNTIME)
public @interface UserLoginToken {
    boolean required() default true;
}


 * @Description:配置拦截器
 */
@Configuration
public class InterceptorConfig implements WebMvcConfigurer {
    @Override
    public void addInterceptors(InterceptorRegistry registry) {
        registry.addInterceptor(authenticationInterceptor())
                .addPathPatterns("/**");// 拦截所有请求，通过判断是否有 @LoginRequired 注解 决定是否需要登录
    }
    @Bean
    public AuthenticationInterceptor authenticationInterceptor() {
        return new AuthenticationInterceptor();
    }
}

```

sso登录以后，可以将Cookie的域设置为顶域，即.a.com，这样所有子域的系统都可以访问到顶域的Cookie。我们在设置Cookie时，只能设置顶域和自己的域，不能设置其他的域。比如：我们不能在自己的系统中给baidu.com的域设置Cookie。

Cookie的问题解决了，我们再来看看session的问题。我们在sso系统登录了，这时再访问app1，Cookie也带到了app1的服务端（Server），app1的服务端怎么找到这个Cookie对应的Session呢？这里就要把3个系统的Session共享，如图所示。共享Session的解决方案有很多，例如：Spring-Session，Redis。这样第2个问题也解决了。

同域下的单点登录是巧用了Cookie顶域的特性。如果是不同域呢？不同域之间Cookie是不共享的，怎么办？

上面是同域名下的单点登录，如果是不同域名下的呢



>
>
>拦截器和过滤器的区别：
>
>​         ①拦截器是基于java的反射机制的，而过滤器是基于函数回调。
>　　②拦截器不依赖与servlet容器，过滤器依赖与servlet容器。
>　　③拦截器只能对action请求起作用，而过滤器则可以对几乎所有的请求起作用。
>　　④拦截器可以访问action上下文、值栈里的对象，而过滤器不能访问。
>　　⑤在action的生命周期中，拦截器可以多次被调用，而过滤器只能在容器初始化时被调用一次
>
>​		⑥拦截器可以获取IOC容器中的各个bean，而过滤器就不行，这点很重要，在拦截器里注入一个service，可以调用业务逻辑。
>
>1.过滤器和拦截器触发时机不一样:
>
>　　过滤器是在请求进入容器后，但请求进入servlet之前进行预处理的。请求结束返回也是，是在servlet处理完后，返回给前端之前。
>
>1.过滤器是JavaEE标准，采用函数回调的方式进行。是在请求进入容器之后，还未进入Servlet之前进行预处理，并且在请求结束返回给前端这之间进行后期处理。
>
>​      拦截器是被包裹在过滤器之中的。
>
>​       还有，拦截器是spring容器的，是spring支持的，
>
>![img](image/%E9%9D%A2%E7%BB%8F/330611-20171023144517066-24770749.png)
>
>![img](image/%E9%9D%A2%E7%BB%8F/330611-20171023150603801-320372296.png)
>
>![img](https://images2017.cnblogs.com/blog/330611/201710/330611-20171023150730676-300068307.png)







CAS实现单点登录

1. 用户访问app系统，app系统是需要登录的，但用户现在没有登录。通过判断请求中有没有ticket或者cookie判断，有ticket表示sso登录了，从那里跳转过来的。
2. 跳转到CAS server，即SSO登录系统，以后图中的CAS Server我们统一叫做SSO系统。 SSO系统也没有登录，弹出用户登录页。跳转到CAS的时候，将app系统的地址作为参数传递过去。如果sso登录过了，请求中会有cookie，这时候需要更新session过期时间
3. 用户填写用户名、密码，SSO系统进行认证后，将登录状态写入SSO的session，浏览器（Browser）中写入SSO域下的Cookie。（在sso系统中还可以将sessionId保存到共享的redis中,key是ticket,value是sessionId,后面加上过期时间）
4. SSO系统登录完成后会生成一个ST（Service Ticket）ST是SSO系统的sessionId的key也可以直接就是sessionID，然后跳转到app系统，同时将ST作为参数传递给app系统。
5. app系统拿到ST后，app的过滤器发现有ST，交给下一个过滤器处理，从后台向SSO发送http请求，验证ST是否有效。如果有效，就把session实体返回给app系统。（如果使用了redis的话，这个时候可以向redis中查找ticket是否存在，获取sessionId，这个过程就相当于验证了ticket的有效性。）
6. 验证通过后，app系统将session实体写入session并设置app域下的Cookie。然后重定向到原访问的URL

至此，跨域单点登录就完成了。以后我们再访问app系统时，app就是登录的。接下来，我们再看看访问app2系统时的流程。

1. 用户访问app2系统，没有携带cookie，并且请求中也不存在ticket，则认为该系统未登录，返回302重定向SSO，并在URL中追加app2的地址。
2. 访问重定向的地址，这个时候SSO系统是可以携带cookie的，由于SSO已经登录了，不需要重新登录认证。
3. SSO生成ST，浏览器跳转到app2系统，并将ST作为参数传递给app2。
4. app2拿到ST，此时请求将会进入项目app的ticket认证Filter中,后台访问SSO，验证ST是否有效,验证成功将返回session给app2。
5. 验证成功后，app2将登录状态写入session，并在app2域下写入Cookie。最终将请求重定向到原访问项目app2的地址

这样，app2系统不需要走登录流程，就已经是登录了。SSO，app和app2在不同的域，它们之间的session不共享也是没问题的。

有的同学问我，SSO系统登录后，跳回原业务系统时，带了个参数ST，业务系统还要拿ST再次访问SSO进行验证，觉得这个步骤有点多余。他想SSO登录认证通过后，通过回调地址将用户信息返回给原业务系统，原业务系统直接设置登录状态，这样流程简单，也完成了登录，不是很好吗？

其实这样问题时很严重的，如果我在SSO没有登录，而是直接在浏览器中敲入回调的地址，并带上伪造的用户信息，是不是业务系统也认为登录了呢？这是很可怕的。

当访问CAS注销地址后：

1.清除位于客户端浏览器进程所占用的内存中的TGC Cookie (设空)

2.清除位于CAS Server中对应的TGT。

3.通过HTTP请求分别通知当前用户所有已登录的CAS Client进行注销登录操作，此时请求将会进入CAS Client的单点登出Filter，单点登出Filter中判断当前请求是否是POST请求方式并且是否携带了logoutRequest参数，若不属于则放行，将请求交给下一个过滤器进行处理，若属于则进行Session对象的销毁。

 

*****当注销后，TGC、TGT、CAS Client用户对应的Session对象将会失效，此时再访问项目A和项目B需要重新登录。

![image-20200727163246099](image/%E9%9D%A2%E7%BB%8F/image-20200727163246099.png)

![image-20200727163501342](image/%E9%9D%A2%E7%BB%8F/image-20200727163501342.png)

![image-20200727163556318](image/%E9%9D%A2%E7%BB%8F/image-20200727163556318.png)

![image-20200727163612838](image/%E9%9D%A2%E7%BB%8F/image-20200727163612838.png)

一、oauth2与单点登陆的区别

1、oauth2，不同的企业之间的登陆，应用之间的信任度较低

2、单点登陆，是同一企业的产品系列间的登陆，相互信任度较高

二、session-cookie机制

1、session-cookie机制出现的根源， http连接是无状态的连接

-------- 同一浏览器向服务端发送多次请求，服务器无法识别，哪些请求是同一个浏览器发出的

![img](image/%E9%9D%A2%E7%BB%8F/wpsKy15c6.jpg) 

2、为了标识哪些请求是属于同一个人 ---------- 需要在请求里加一个标识参数

方法1-----------直接在url里加一个标识参数(对前端开发有侵入性)，如: token

方法2-----------http请求时，自动携带浏览器的cookie（对前端开发无知觉），如：jsessionid=dfdfdfdfdf 

3、浏览器标识在网络上的传输，是明文的，不安全的

-----------安全措施：改https来保障

4、cookie的使用限制---依赖域名

-------------- 顶级域名下cookie，会被二级以下的域名请求，自动携带

-------------- 二级域名的cookie，不能携带被其它域名下的请求携带

5、在服务器后台，通过解读标识信息（token或jsessionid），来对应会话是哪个session

--------------- 一个tomcat，被1000个用户登陆，tomcat里一定有1000个session -------》存储格式map《sessionid，session对象》

--------------- 通过前端传递的jsessionid，来对应取的session ------ 动作发生时机request.getsession

三、session共享方式，实现的单点登陆

1、多个应用共用同一个顶级域名，sessionid被种在顶级域名的cookie里

2、后台session通过redis实现共享，即每个tomcat都在请求开始时，到redis查询session;在请求返回时，将自身session对象存入redis

3、当请求到达服务器时，服务器直接解读cookie中的sessionid，然后通过sessionid到redis中查找到对应会话session对象

4、后台判断请求是否已登陆，主要校验session对象中，是否存在登陆用户信息

5、整个校验过程，通过filter过滤器来拦截切入，如下图：

![img](image/%E9%9D%A2%E7%BB%8F/wpsImQWvd.jpg) 

6、登陆成功时，后台需要给页面种cookie方法如下：

![img](image/%E9%9D%A2%E7%BB%8F/wpsQDFOOk.jpg) 

response里，反映的种cookie效果如下：

![img](image/%E9%9D%A2%E7%BB%8F/wpseAAH7r.jpg) 

7、为了request.getsession时，自动能拿到redis中共享的session，

  我们需要重写request的getsession方法（使用HttpServletRequestWrapper包装原request）

四、cas单点登陆方案

1、对于完全不同域名的系统，cookie是无法跨域名共享的

2、cas方案，直接启用一个专业的用来登陆的域名（比如：cas.com）来供所有的系统登陆。

3、当业务系统（如b.com）被打开时，借助cast系统来登陆，过程如下：

cas登陆的全过程：

（1）、b.com打开时，发现自己未登陆 ----》 于是跳转到cas.com去登陆,（根据有没有cookie判断cas有没有登录，如果CAS登录了，直接生成ticket返回就可以了，把ticket和sessionId保存到redis中）

（2）、cas.com登陆页面被打开，用户输入帐户/密码登陆成功 

（3）、cas.com登陆成功，种cookie到cas.com域名下 -----------》把sessionid放入后台redis《ticket，sesssionid》---页面跳回b.com

（4）、b.com重新被打开，发现仍然是未登陆，但是有了一个ticket值

（5）、b.com用ticket值，到redis里查到sessionid，并做session同步 ------ 》种cookie给自己，页面原地重跳

（6）、b.com打开自己页面，此时有了cookie，后台校验登陆状态，成功则返回session实体。

如果每个系统都有自己单独的redis，这个时候就不要共享redis保存ticket了，而是b.com拿着ticket去cas.com校验，子啊cas中ticket是保存到一个hashmap中的,验证成功之后将

（7）整个过程交互，列图如下：

<img src="image/%E9%9D%A2%E7%BB%8F/wpsm1jCqz.jpg" alt="img" style="zoom:150%;" /> 

4、cas.com的登陆页面被打开时,如果此时cas.com本来就是登陆状态的,则自动返回生成ticket给业务系统

​	整个单点登陆的关键部位，是利用cas.com的cookie保持cas.com是登陆状态,此后任何第三个系统跳入,都将自动完成登陆过程

5,本示例中,使用了redis来做cas的服务接口,请根据工作情况,自行替换为合适的服务接口(主要是根据sessionid来判断用户是否已登陆)

6,为提高安全性,ticket应该使用过即作废(本例中,会用有效期机制)